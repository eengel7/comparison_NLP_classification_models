{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#! git clone https://github.com/eengel7/comparison_NLP_classification_models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/comparison_NLP_classification_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meengel7\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_data.pkl already exists at data/preprocessed/bert_en_all_levels_val.\n",
      "Data already exists and will not be overwritten.\n"
     ]
    }
   ],
   "source": [
    "from src.classification import (\n",
    "    MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from src.training import train_model\n",
    "from src.evaluation import eval_model\n",
    "from src.preprocessing.get_preprocessed_data import get_preprocessed_data\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "from src.utils import prepare_df\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Experiment config:----------------------\n",
    "model_type = 'bert'\n",
    "model_name = \"bert-base-uncased\" # 'bert-base-uncased' ,\"roberta-base\", \"outputs/checkpoint-120-epoch-1\",\n",
    "\n",
    "model_args = MultiLabelClassificationArgs(\n",
    "                                          wandb_project ='multi-label-42', \n",
    "                                          wandb_kwargs = {\"name\": model_name},\n",
    "                                          learning_rate = 5e-5,\n",
    "                                          num_train_epochs=10,\n",
    "                                          train_batch_size = 4,\n",
    "                                          eval_batch_size = 4,\n",
    "                                          evaluate_during_training= True, \n",
    "                                          use_multiprocessing= True,\n",
    "                                          use_early_stopping= True,\n",
    "                                          early_stopping_patience=3,\n",
    "                                          early_stopping_delta= 1e-5\n",
    "                                          ) \n",
    "model = MultiLabelClassificationModel(\n",
    "    model_type,\n",
    "    model_name,\n",
    "    num_labels=305,\n",
    "    args=model_args,\n",
    "    # use_cuda = True\n",
    ")\n",
    "# -----------------------------------\n",
    "# Prepare data\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = get_preprocessed_data(model_type, overwrite_data= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded. Training starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/26877 [00:31<23:26:47,  3.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    854\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m val_df \u001b[39m=\u001b[39m prepare_df(X_val, Y_val)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mData is loaded. Training starts.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m train_model(model, train_df, eval_df \u001b[39m=\u001b[39;49m val_df)\n\u001b[1;32m      8\u001b[0m \u001b[39m# # Evaluate the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# result, model_outputs, wrong_predictions = eval_model(model,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#     test\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# print(predictions,raw_outputs)\u001b[39;00m\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/training.py:104\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(classification_model, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     train_examples \u001b[39m=\u001b[39m (\n\u001b[1;32m    101\u001b[0m         train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    102\u001b[0m         train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 104\u001b[0m train_dataset \u001b[39m=\u001b[39m classification_model\u001b[39m.\u001b[39;49mload_and_cache_examples(\n\u001b[1;32m    105\u001b[0m     train_examples, verbose\u001b[39m=\u001b[39;49mverbose\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m train_sampler \u001b[39m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    108\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    109\u001b[0m     train_dataset,\n\u001b[1;32m    110\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    111\u001b[0m     batch_size\u001b[39m=\u001b[39mclassification_model\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    112\u001b[0m     num_workers\u001b[39m=\u001b[39mclassification_model\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/multi_label_classification_model.py:315\u001b[0m, in \u001b[0;36mMultiLabelClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_and_cache_examples\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m     examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ):\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload_and_cache_examples(\n\u001b[1;32m    316\u001b[0m         examples,\n\u001b[1;32m    317\u001b[0m         evaluate\u001b[39m=\u001b[39;49mevaluate,\n\u001b[1;32m    318\u001b[0m         no_cache\u001b[39m=\u001b[39;49mno_cache,\n\u001b[1;32m    319\u001b[0m         multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    320\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    321\u001b[0m         silent\u001b[39m=\u001b[39;49msilent,\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/classification_model.py:375\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m    371\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mcache_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m evaluate \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m dataset \u001b[39m=\u001b[39m ClassificationDataset(\n\u001b[1;32m    376\u001b[0m     examples,\n\u001b[1;32m    377\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m    379\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    380\u001b[0m     multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    381\u001b[0m     output_mode\u001b[39m=\u001b[39;49moutput_mode,\n\u001b[1;32m    382\u001b[0m     no_cache\u001b[39m=\u001b[39;49mno_cache,\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/classification_utils.py:242\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m build_classification_dataset(\n\u001b[1;32m    243\u001b[0m         data, tokenizer, args, mode, multi_label, output_mode, no_cache\n\u001b[1;32m    244\u001b[0m     )\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/classification_utils.py:209\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    203\u001b[0m         data \u001b[39m=\u001b[39m [\n\u001b[1;32m    204\u001b[0m             (text_a[i : i \u001b[39m+\u001b[39m chunksize], \u001b[39mNone\u001b[39;00m, tokenizer, args\u001b[39m.\u001b[39mmax_seq_length)\n\u001b[1;32m    205\u001b[0m             \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    206\u001b[0m         ]\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(args\u001b[39m.\u001b[39mprocess_count) \u001b[39mas\u001b[39;00m p:\n\u001b[0;32m--> 209\u001b[0m         examples \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    210\u001b[0m             tqdm(\n\u001b[1;32m    211\u001b[0m                 p\u001b[39m.\u001b[39;49mimap(preprocess_data_multiprocessing, data),\n\u001b[1;32m    212\u001b[0m                 total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(text_a),\n\u001b[1;32m    213\u001b[0m                 disable\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49msilent,\n\u001b[1;32m    214\u001b[0m             )\n\u001b[1;32m    215\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     examples \u001b[39m=\u001b[39m {\n\u001b[1;32m    218\u001b[0m         key: torch\u001b[39m.\u001b[39mcat([example[key] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples])\n\u001b[1;32m    219\u001b[0m         \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m examples[\u001b[39m0\u001b[39m]\n\u001b[1;32m    220\u001b[0m     }\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/.venv/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 858\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    859\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df = prepare_df(X_train, Y_train)\n",
    "test_df = prepare_df(X_test, Y_test)\n",
    "val_df = prepare_df(X_val, Y_val)\n",
    "print('Data is loaded. Training starts.')\n",
    "\n",
    "train_model(model, train_df, eval_df = val_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# result, model_outputs, wrong_predictions = eval_model(model,\n",
    "#     test\n",
    "# )\n",
    "\n",
    "#print(result, model_outputs, wrong_predictions)\n",
    "# # Make predictions with the model\n",
    "# predictions, raw_outputs = model.predict([\"Sam\"])\n",
    "\n",
    "# print(predictions,raw_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_data.pkl already exists at data/preprocessed/transformer_en_all_levels_val_42.\n",
      "Data already exists and will not be overwritten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiLabelSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "4135   Developing strategies for scientific excellenc...   \n",
      "34132  Feasibility study for implementation of AI-bas...   \n",
      "38440  Internet of Things Initiative\\nInternet of Thi...   \n",
      "42733  Advancing cancer detection through metabolism-...   \n",
      "9764   Sustainable manufacturing of 3D printed sandmo...   \n",
      "...                                                  ...   \n",
      "18507  From assistant to associate professor: Career ...   \n",
      "4346   New Smart Nanoparticle Materials: Development ...   \n",
      "21120  Participation in the JPI Oceans Pilot Action ´...   \n",
      "34511  A New Region of the World?\\nA New Region of th...   \n",
      "37371  NeoCel – Novel processes for sustainable cellu...   \n",
      "\n",
      "                                                  labels  \n",
      "4135   [1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...  \n",
      "34132  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "38440  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "42733  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "9764   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                  ...  \n",
      "18507  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4346   [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
      "21120  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
      "34511  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "37371  [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[41997 rows x 2 columns]\n",
      "Training starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/41997 [02:09<31:24:09,  2.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    854\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mprint\u001b[39m(combined_df)\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining starts.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m train_model(model, combined_df)\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/training.py:104\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(classification_model, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     train_examples \u001b[39m=\u001b[39m (\n\u001b[1;32m    101\u001b[0m         train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    102\u001b[0m         train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 104\u001b[0m train_dataset \u001b[39m=\u001b[39m classification_model\u001b[39m.\u001b[39;49mload_and_cache_examples(\n\u001b[1;32m    105\u001b[0m     train_examples, verbose\u001b[39m=\u001b[39;49mverbose\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m train_sampler \u001b[39m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    108\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    109\u001b[0m     train_dataset,\n\u001b[1;32m    110\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    111\u001b[0m     batch_size\u001b[39m=\u001b[39mclassification_model\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    112\u001b[0m     num_workers\u001b[39m=\u001b[39mclassification_model\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/multi_label_classification_model.py:315\u001b[0m, in \u001b[0;36mMultiLabelClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_and_cache_examples\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    308\u001b[0m     examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ):\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload_and_cache_examples(\n\u001b[1;32m    316\u001b[0m         examples,\n\u001b[1;32m    317\u001b[0m         evaluate\u001b[39m=\u001b[39;49mevaluate,\n\u001b[1;32m    318\u001b[0m         no_cache\u001b[39m=\u001b[39;49mno_cache,\n\u001b[1;32m    319\u001b[0m         multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    320\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    321\u001b[0m         silent\u001b[39m=\u001b[39;49msilent,\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/classification_model.py:375\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m    371\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mcache_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m evaluate \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m dataset \u001b[39m=\u001b[39m ClassificationDataset(\n\u001b[1;32m    376\u001b[0m     examples,\n\u001b[1;32m    377\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m    379\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    380\u001b[0m     multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m    381\u001b[0m     output_mode\u001b[39m=\u001b[39;49moutput_mode,\n\u001b[1;32m    382\u001b[0m     no_cache\u001b[39m=\u001b[39;49mno_cache,\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/classification_utils.py:253\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m build_classification_dataset(\n\u001b[1;32m    254\u001b[0m         data, tokenizer, args, mode, multi_label, output_mode, no_cache\n\u001b[1;32m    255\u001b[0m     )\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/src/classification/classification_utils.py:220\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    214\u001b[0m         data \u001b[39m=\u001b[39m [\n\u001b[1;32m    215\u001b[0m             (text_a[i : i \u001b[39m+\u001b[39m chunksize], \u001b[39mNone\u001b[39;00m, tokenizer, args\u001b[39m.\u001b[39mmax_seq_length)\n\u001b[1;32m    216\u001b[0m             \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    217\u001b[0m         ]\n\u001b[1;32m    219\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(args\u001b[39m.\u001b[39mprocess_count) \u001b[39mas\u001b[39;00m p:\n\u001b[0;32m--> 220\u001b[0m         examples \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    221\u001b[0m             tqdm(\n\u001b[1;32m    222\u001b[0m                 p\u001b[39m.\u001b[39;49mimap(preprocess_data_multiprocessing, data),\n\u001b[1;32m    223\u001b[0m                 total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(text_a),\n\u001b[1;32m    224\u001b[0m                 disable\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49msilent,\n\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m         )\n\u001b[1;32m    228\u001b[0m     examples \u001b[39m=\u001b[39m {\n\u001b[1;32m    229\u001b[0m         key: torch\u001b[39m.\u001b[39mcat([example[key] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples])\n\u001b[1;32m    230\u001b[0m         \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m examples[\u001b[39m0\u001b[39m]\n\u001b[1;32m    231\u001b[0m     }\n\u001b[1;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/comparison_NLP_classification_models/.venv/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 858\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    859\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.preprocessing.get_preprocessed_data import get_preprocessed_data\n",
    "import logging\n",
    "\n",
    "import wandb\n",
    "from config.data_args import DataArgs\n",
    "from src.classification import (MultiLabelClassificationArgs,\n",
    "                                MultiLabelClassificationModel)\n",
    "from src.evaluation import eval_model\n",
    "from src.preprocessing.get_preprocessed_data import get_preprocessed_data\n",
    "from src.training import train_model\n",
    "from src.utils import prepare_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = get_preprocessed_data('bert', overwrite_data= False, random_seed= 42)  \n",
    "\n",
    "\n",
    "# Initialise the model\n",
    "model_args = MultiLabelClassificationArgs(\n",
    "                                        learning_rate = 5e-5,\n",
    "                                        num_train_epochs=15,\n",
    "                                        train_batch_size = 4,\n",
    "                                        use_multiprocessing= False,\n",
    "                                        use_early_stopping= True,\n",
    "                                        early_stopping_patience=3,\n",
    "                                        early_stopping_delta= 0\n",
    "                                        ) \n",
    "\n",
    "model = MultiLabelClassificationModel( \n",
    "    'bert',\n",
    "    'bert-base-uncased',\n",
    "    num_labels=305,\n",
    "    use_cuda = False\n",
    ")\n",
    "train_df = prepare_df(X_train, Y_train)\n",
    "val_df = prepare_df(X_val, Y_val)\n",
    "test_df = prepare_df(X_test, Y_test)\n",
    "\n",
    "combined_df = pd.concat([train_df, val_df, test_df], axis=0)\n",
    "# Training\n",
    "print(combined_df)\n",
    "print('Training starts.')\n",
    "train_model(model, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9883284e4809f9dc439720b7be050568a57fcaf846432ff7264a79bbda205b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
